{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/luke/anaconda2/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score, mean_squared_error\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import time\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('max_columns',500)\n",
    "font = {'size': 20}\n",
    "rc('font', **font)\n",
    "plt.style.use('seaborn-bright')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/SRP/clean_data_public_no_crime_lag2_by_store.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_table_clust = pd.read_csv('data/SRP/cust_table.pkl')\n",
    "df = df.join(cust_table_clust, on='address1', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into features vs targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y(df, non_feature_cols, target_col):\n",
    "    non_feature_data = df[non_feature_cols]\n",
    "    features = list(set(df) - set(non_feature_cols))\n",
    "    features.sort()\n",
    "    X = df[features]\n",
    "    y = non_feature_data[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split(df, date_col, date, non_feature_cols, target_col):\n",
    "    df_train = df[ df[date_col] < date ]\n",
    "    df_test = df[ df[date_col] >= date ]\n",
    "    X_train, y_train = X_y(df_train, non_feature_cols, target_col)\n",
    "    X_test, y_test = X_y(df_test, non_feature_cols, target_col)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and targets\n",
    "non_feature_cols = ['shrink_value', 'shrink_to_sales_value_pct', 'shrink_value_out', 'shrink_to_sales_value_pct_out',\n",
    "               'shrink_value_ex_del', 'shrink_to_sales_value_pct_ex_del', 'qty_inv_out', 'qty_shrink',\n",
    "               'qty_shrink_ex_del', 'qty_shrink_out', 'qty_end_inventory', 'qty_f', 'qty_out', 'qty_ex_del',\n",
    "               'qty_n', 'qty_delivery', 'qty_o', 'qty_d', 'qty_shrink_per_day', 'shrink_value_per_day']\n",
    "X, y = X_y(df, non_feature_cols, target_col='shrink_value_per_day')\n",
    "# del df # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split for model testing (try/except based on customer seg being completed)\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=X.cluster.values)\n",
    "except:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# time split for forcast\n",
    "split_date = pd.to_datetime('12/01/2017')\n",
    "forc_X_train, forc_X_test, forc_y_train, forc_y_test = time_split(df, 'visit_date', split_date, \n",
    "                                                                  non_feature_cols, target_col='shrink_value_per_day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_f(X_std):\n",
    "    std_mask = (X_std.dtypes == int) | (X_std.dtypes == np.float64) # only standardize numbers that are not associated with time features\n",
    "    std_cols = X_std.columns[std_mask]\n",
    "    ss = StandardScaler()\n",
    "    X_std[std_cols] = ss.fit_transform(X_std[std_cols])\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_f(X_sc):\n",
    "    sc_mask = (X_sc.dtypes == np.float32) # only scale time features\n",
    "    sc_cols = X_sc.columns[sc_mask]\n",
    "    min_time = X_sc[sc_cols].min().values.min()\n",
    "    max_time = X_sc[sc_cols].max().values.max()\n",
    "    for col in sc_cols:\n",
    "        # scale all time features using the same two values, so equivalent values reference the same date across columns\n",
    "        X_sc[col] = (X_sc[col] - min_time) / (max_time - min_time)\n",
    "    return X_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss(X_ss, std=True, scale=True):\n",
    "    if not std and not scale:\n",
    "        return\n",
    "    X_new = X_ss.copy()\n",
    "    if std:\n",
    "        X_new = std_f(X_new)\n",
    "    if scale:\n",
    "        X_new = scale_f(X_new)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standardize and scale data\n",
    "X_train = ss(X_train, std=True, scale=True)\n",
    "X_test = ss(X_test, std=True, scale=True)\n",
    "forc_X_train = ss(forc_X_train, std=True, scale=True)\n",
    "forc_X_test = ss(forc_X_test, std=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask of all numberical columns to be used in clustering/modeling\n",
    "\n",
    "# including time features\n",
    "numb_mask = (X_train.dtypes == int) | (X_train.dtypes == np.float64) | (X_train.dtypes == np.float32) | (X_train.dtypes == np.uint8)\n",
    "numb_cols = X_train.columns[numb_mask]\n",
    "\n",
    "# not including time features\n",
    "numb_no_time_mask = (X_train.dtypes == int) | (X_train.dtypes == np.float64) | (X_train.dtypes == np.uint8)\n",
    "numb_no_time_cols = X_train.columns[numb_no_time_mask]\n",
    "\n",
    "# forcasting columns (what is known months ahead of time)\n",
    "forc_cols = ['FD_ratio', 'LAPOP1_10', 'POP2010', 'customer_id_1635139',\n",
    "             'customer_id_1903139', 'customer_id_2139', 'customer_id_2331150',\n",
    "             'customer_id_2741156', 'customer_id_2773156', 'customer_id_2782156',\n",
    "             'customer_id_2956160', 'customer_id_2977160', 'customer_id_3083182',\n",
    "             'customer_id_3088198', 'customer_id_3088201', 'customer_id_3089336',\n",
    "             'customer_id_3093327', 'customer_id_3093329', 'customer_id_3097348',\n",
    "             'dens_sq_mile', 'shrink_value_per_day_lag1', 'shrink_value_per_day_lag2', 'unemp_rate']\n",
    "# mask to be used in calculations\n",
    "model_mask_cols = numb_no_time_cols"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# sb.pairplot(df.sample(1000).filter(numb_cols))\n",
    "mask = (df.dtypes == int) | (df.dtypes == np.float64) | (df.dtypes == np.float32)\n",
    "i = 1\n",
    "fig = plt.figure(figsize=(6, 120))\n",
    "columns = df.columns[mask]\n",
    "for col in columns:\n",
    "    ax = fig.add_subplot(len(columns), 1, i)\n",
    "    ax.scatter(df[col].values, df['qty_shrink'].values)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('qty_shrink')\n",
    "    i += 1\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "    - PCA\n",
    "    - SVD\n",
    "    - Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var(pca):\n",
    "    '''\n",
    "    Input: fitted PCA\n",
    "    '''\n",
    "    var_arr = np.insert(pca.explained_variance_ratio_, [0], 0)\n",
    "    cum_arr = np.cumsum(var_arr)\n",
    "    feat_arr = np.arange(0, len(var_arr), 1)\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(feat_arr, var_arr, label='Variance at each point', c='r')\n",
    "    plt.plot(feat_arr, cum_arr, label='Cumulative variance')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylabel('Fraction of total variance explained')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.xticks(feat_arr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(10)\n",
    "pca.fit(X_train[model_mask_cols])\n",
    "plot_var(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=10)\n",
    "svd.fit(X_train[model_mask_cols])\n",
    "plot_var(svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Models\n",
    "Clustering models to try:\n",
    "    - k-means\n",
    "    - Heirarchal clustering\n",
    "        - Look into the neat visualizations in R!\n",
    "Link [here](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning)\n",
    "    - Neural net\n",
    "Perform for both subjective (features only) and objective (including targets/possibly even only targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['theft/person'] = df['2015_theft_count'] / df['POP2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:18: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "cust_table = df.groupby(['address1']).mean()[['qty_shrink_per_day', 'shrink_value_per_day', 'POP2010',\n",
    "                                              'FD_ratio', 'unemp_rate', 'dens_sq_mile', ]].reset_index()\n",
    "cust_table.set_index('address1', inplace=True)\n",
    "\n",
    "city_i = df.columns.get_loc('city')\n",
    "state_i = df.columns.get_loc('state')\n",
    "zip_i = df.columns.get_loc('zip_code')\n",
    "cust_i = df.columns.get_loc('customer_id')\n",
    "for index, row in cust_table.iterrows():\n",
    "    foo = df[ df.address1 == index]\n",
    "    for i, r in foo.iterrows():\n",
    "        city = r[city_i]\n",
    "        state = r[state_i]\n",
    "        zip_code = r[zip_i]\n",
    "        cust_id = r[cust_i]\n",
    "        \n",
    "        cust_table.set_value(index, 'city', city)\n",
    "        cust_table.set_value(index, 'state', state)\n",
    "        cust_table.set_value(index, 'zip_code', zip_code)\n",
    "        cust_table.set_value(index, 'customer_id', cust_id)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cust = pd.get_dummies(cust_table, columns=['customer_id','zip_code'])\n",
    "# including shrink and not inluding dummies\n",
    "shrink_cust_mask = (dummy_cust.dtypes == float)\n",
    "shrink_cust_cols = dummy_cust.columns[shrink_cust_mask]\n",
    "\n",
    "# including dummies but not shrink\n",
    "dummy_cust_mask = (dummy_cust.dtypes == float) | (dummy_cust.dtypes == np.uint8)\n",
    "dummy_cust_cols = dummy_cust.columns[dummy_cust_mask]\n",
    "dummy_cust_cols = list(dummy_cust_cols)\n",
    "dummy_cust_cols.remove('qty_shrink_per_day')\n",
    "dummy_cust_cols.remove('shrink_value_per_day')\n",
    "\n",
    "# including dummies and shrink\n",
    "all_cust_mask = (dummy_cust.dtypes == float) | (dummy_cust.dtypes == np.uint8)\n",
    "all_cust_cols = dummy_cust.columns[dummy_cust_mask]\n",
    "\n",
    "std_cust = std_f(dummy_cust.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before regularization: ',std_cust[dummy_cust_cols].shape)\n",
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(std_cust[dummy_cust_cols], std_cust['shrink_value_per_day'])\n",
    "model = SelectFromModel(lasso, prefit=True)\n",
    "std_cust_reduc = model.transform(std_cust[dummy_cust_cols])\n",
    "print('Shape after regularization: ',std_cust_reduc.shape)\n",
    "std_cust_reduc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before regularization: ',std_cust[dummy_cust_cols].shape)\n",
    "lsvr = LinearSVR(C=0.01, loss='epsilon_insensitive', dual=True)\n",
    "lsvr.fit(std_cust[dummy_cust_cols], std_cust['shrink_value_per_day'])\n",
    "model = SelectFromModel(lsvr, prefit=True)\n",
    "std_cust_reduc = model.transform(std_cust[dummy_cust_cols])\n",
    "print('Shape after regularization: ',std_cust_reduc.shape)\n",
    "std_cust_reduc\n",
    "model.get_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cust_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# columns to use in segmentation:\n",
    "pca_cols = ['qty_shrink_per_day', 'shrink_value_per_day', 'FD_ratio', 'dens_sq_mile', 'POP2010', 'unemp_rate']\n",
    "clusters = np.arange(1, 15)\n",
    "SSE_arr, ss_arr = kmeans(std_cust[all_cust_cols], clusters)\n",
    "#elbow(clusters, SSE_arr)\n",
    "silhouette(np.arange(2, 15), ss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_kmeans = KMeans(n_clusters=5, max_iter=10000, n_jobs=-1)\n",
    "pred = cust_kmeans.fit_predict(std_cust[all_cust_cols])\n",
    "dummy_cust['cluster'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy_cust.groupby('cluster').count().city)\n",
    "dummy_cust.groupby('cluster').mean()[all_cust_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pca = PCA(2)\n",
    "pcas = cust_pca.fit_transform(std_cust[all_cust_cols])\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(pcas[:,0], pcas[:,1], c=dummy_cust.cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pca = PCA(3)\n",
    "pcas = cust_pca.fit_transform(std_cust[all_cust_cols])\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(pcas[:,0], pcas[:,1], pcas[:,2], s=20, alpha=1, c=dummy_cust.cluster)\n",
    "ax.set_xlim(left=-10, right=2)\n",
    "ax.set_ylim(bottom=0, top=10)\n",
    "ax.set_zlim(top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cust_table_clust = cust_table[['cluster']].astype(str)\n",
    "cust_table_clust = dummy_cust[['cluster']].astype(str)\n",
    "cust_table_clust.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X_km, clusters):\n",
    "    SSE_arr = []\n",
    "    ss_arr = []\n",
    "    for i in clusters:\n",
    "        kmeans = KMeans(n_clusters=i, n_jobs=-1)\n",
    "        clust_dist = kmeans.fit_transform(X_km)\n",
    "        clust_num = kmeans.predict(X_km)\n",
    "\n",
    "        SSE = 0\n",
    "        for a, b in zip(clust_dist, clust_num):\n",
    "            SSE += a[b] ** 2\n",
    "        SSE_arr.append(SSE)\n",
    "        \n",
    "        if i > 1:\n",
    "            ss_arr.append(silhouette_score(X_km, clust_num))\n",
    "    return SSE_arr, ss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters = np.arange(1, 20)\n",
    "SSE_arr, ss_arr = kmeans(X_train[model_mask_cols], clusters)\n",
    "elbow(clusters, SSE_arr)\n",
    "silhouette(np.arange(2, 20), ss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow(clusters, SSE_arr):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Elbow Plot')\n",
    "    plt.plot(clusters, SSE_arr)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(clusters)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Sum of Squares Error (SSE)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(clusters, ss_arr):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title('Silhouette Scores')\n",
    "    plt.plot(clusters, ss_arr)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(clusters)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heir_clust(X_hc, thresh, dist_metric='cosine', num_params_to_display=50):\n",
    "    # Find distances using pair-wise distances in the array, according to desired metric\n",
    "    dist = squareform(pdist(X_hc.values.T, metric = dist_metric))\n",
    "\n",
    "    # Plot dendrogram\n",
    "    fig, axarr = plt.subplots(nrows = 3, ncols = 1, figsize=(60, 80))\n",
    "    for ax, linkmethod in zip(axarr.flatten(), ['single', 'complete', 'average']):\n",
    "        clust = linkage(dist, method=linkmethod)\n",
    "        dendrogram(clust, ax=ax, truncate_mode='lastp', p=num_params_to_display, labels=model_mask_cols, \n",
    "                   color_threshold=thresh, leaf_font_size=25) #color threshold number sets the color change\n",
    "        ax.set_title('{} linkage'.format(linkmethod), fontsize=40)\n",
    "        ax.grid(alpha=0.3)\n",
    "    plt.savefig('images/clust.png'.format(linkmethod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heir_clust(X_train[model_mask_cols], thresh=1.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models\n",
    "Regression models to try:\n",
    "    - Linear regression (with additional complexity)\n",
    "    - Random forest\n",
    "    - Boosting\n",
    "    - Gradient descent\n",
    "    - Neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols(X_train, X_test, y_train, y_test):\n",
    "    ols = OLS(y_train, add_constant(X_train.values, has_constant='add'))\n",
    "    result = ols.fit()\n",
    "    pred = result.predict(add_constant(X_test.values, has_constant='add'))\n",
    "    score = mean_squared_error(y_test, pred)\n",
    "    print('Root Mean Square Error: ',score)\n",
    "    names = list(X_train.columns)\n",
    "    names.insert(0,'Constant')\n",
    "    print(result.summary(xname=names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ols(X_train[model_mask_cols], X_test[model_mask_cols], y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for val in X_train.item_category.unique():\n",
    "    print('Item cat: ', val)\n",
    "    mask = X['item_category_{}'.format(val)] == 1\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X[mask], y[mask], test_size=0.2)\n",
    "    size = len(X_train_temp)\n",
    "    print('Size: ', size)\n",
    "    if size < 30:\n",
    "        continue\n",
    "    ols(X_train_temp[model_mask_cols], X_test_temp[model_mask_cols], y_train_temp, y_test_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple Sklear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_crossval(X, y, models, scoring='neg_mean_absolute_error'):\n",
    "    results = []\n",
    "    names = []\n",
    "    all_scores = []\n",
    "    print('Mod - Avg - Std Dev')\n",
    "    print('---   ---   -------')\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=-1)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        print('{}: {:.2f} ({:2f})'.format(name, cv_results.mean(), cv_results.std()))\n",
    "    \n",
    "    fig = plt.figure(figsize=(25, 18))\n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Algorithm Comparison of CrossVal Scores')\n",
    "    ax = fig.add_subplot(111)\n",
    "    sb.violinplot(data=results, orient='v')\n",
    "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax.set_ylabel('K-Fold CV Negative Mean Abs. Error')\n",
    "    ax.set_xlabel('Model')\n",
    "    plt.grid(alpha=0.4)\n",
    "    #plt.savefig('images/model_selection_shrink_value.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initial Cross Validation\n",
    "models = []\n",
    "models.append(('Linear Regression', LinearRegression()))\n",
    "models.append(('Ridge Regression', Ridge()))\n",
    "models.append(('Lasso Regression', Lasso()))\n",
    "models.append(('Elastic Net', ElasticNet()))\n",
    "#models.append(('Stochastic Gradient Descent', SGDRegressor(max_iter=10000, tol=0.001)))\n",
    "#models.append(('Support Vector Regression', SVR(max_iter=10000)))\n",
    "models.append(('K Nearest Neighbors', KNeighborsRegressor(n_jobs=-1)))\n",
    "models.append(('Decision Tree', DecisionTreeRegressor()))\n",
    "models.append(('Random Forest', RandomForestRegressor()))\n",
    "#models.append(('AdaBoost', AdaBoostRegressor(n_estimators=100)))\n",
    "models.append(('Gradient Boost', GradientBoostingRegressor()))\n",
    "models.append(('Multi-Layer Perceptron', MLPRegressor(alpha=1)))\n",
    "\n",
    "class_crossval(X_train[model_mask_cols], y_train, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_grid_plus_error(model, param_grid, X_train, X_test, y_train, y_test):\n",
    "    test_model = model\n",
    "    grid = GridSearchCV(test_model, param_grid=param_grid, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    model.set_params(**best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return grid, mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "params = {'n_estimators': [10, 30], 'max_features': [5, 10, 15], 'max_depth': [None, 20], 'n_jobs': [-1]}\n",
    "grid, mse = model_grid_plus_error(model, params, X_train[model_mask_cols], X_test[model_mask_cols], y_train, y_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor()\n",
    "params = {'hidden_layer_sizes': [(50,50)], 'alpha': [0.000001, 0.00001, 0.0001, 0.001], 'max_iter': [100]}\n",
    "grid, mse = model_grid_plus_error(model, params, X_train[model_mask_cols], X_test[model_mask_cols], y_train, y_test)\n",
    "print(mse)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting By Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dist = df.shrink_value_per_day.values\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(value_dist, bins=2000)\n",
    "#plt.yscale('log')\n",
    "plt.xlim(xmin=-5, xmax=10)\n",
    "plt.xlabel('Shrink Value/Day')\n",
    "\n",
    "print('Average: ', value_dist.mean())\n",
    "print('Std. Dev.: ',value_dist.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.groupby('cluster').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_clusters(model, X_train, X_test, col_mask, y_train, y_test):\n",
    "    cluster_rmse = []\n",
    "    cluster_models = []\n",
    "    for clust in range(0, len(X_train.cluster.unique())):\n",
    "        print(clust)\n",
    "        train_clust_mask = X_train.cluster == str(clust)\n",
    "        test_clust_mask = X_test.cluster == str(clust)\n",
    "        clust_model = model\n",
    "        clust_model.fit(X_train[col_mask][train_clust_mask], y_train[train_clust_mask])\n",
    "        y_pred = clust_model.predict(X_test[col_mask][test_clust_mask])\n",
    "        cluster_rmse.append(np.sqrt(mean_squared_error(y_test[test_clust_mask], y_pred)))\n",
    "        cluster_models.append(clust_model)\n",
    "    return cluster_rmse, cluster_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_rmse, cluster_models = model_clusters(MLPRegressor(alpha=0.00001, hidden_layer_sizes=(50,50)),  \n",
    "                                             X_train, X_test, model_mask_cols,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.bar(np.arange(0,len(X_train.cluster.unique())), cluster_rmse)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Test RMSE')\n",
    "plt.grid(alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What model should look like:\n",
    "1. Take grouped row for one store on one date:\n",
    "    - Run through that store's model to predict shrink_value on that date\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FD_ratio', 'LAPOP1_10', 'POP2010', 'customer_id_1635139',\n",
       "       'customer_id_1903139', 'customer_id_2139', 'customer_id_2331150',\n",
       "       'customer_id_2741156', 'customer_id_2773156', 'customer_id_2782156',\n",
       "       'customer_id_2956160', 'customer_id_2977160', 'customer_id_3083182',\n",
       "       'customer_id_3088198', 'customer_id_3088201', 'customer_id_3089336',\n",
       "       'customer_id_3093327', 'customer_id_3093329', 'customer_id_3097348',\n",
       "       'dens_sq_mile', 'item_category_10', 'item_category_16',\n",
       "       'item_category_19', 'item_category_26', 'item_category_31',\n",
       "       'item_category_38', 'item_category_41', 'item_category_43',\n",
       "       'item_category_44', 'item_category_46', 'item_category_58',\n",
       "       'item_category_62', 'item_category_77', 'item_category_79',\n",
       "       'item_category_8', 'item_category_90', 'qty_POG_limit',\n",
       "       'qty_prev_end_inventory', 'qty_sales',\n",
       "       'qty_shrink_per_day_lag1_by_store', 'qty_shrink_per_day_lag2_by_store',\n",
       "       'qty_start_inventory', 'sales_value',\n",
       "       'shrink_value_per_day_lag1_by_store',\n",
       "       'shrink_value_per_day_lag2_by_store', 'unemp_rate', 'unit_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mask_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forc_X_train.groupby(['address1', 'visit_date']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forc_X_test.groupby(['address1', 'visit_date']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(X, y, batch_size, nb_epoch, neurons):\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]\n",
    " \n",
    "# load dataset\n",
    "series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    " \n",
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    " \n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    " \n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    " \n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    " \n",
    "# repeat experiment\n",
    "repeats = 1\n",
    "error_scores = list()\n",
    "for r in range(repeats):\n",
    "    # fit the model\n",
    "    lstm_model = fit_lstm(X_train[model_mask_cols], y_train, 1, 3, 4)\n",
    "    # forecast the entire training dataset to build up state for forecasting\n",
    "    train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "    lstm_model.predict(train_reshaped, batch_size=1)\n",
    "    # walk-forward validation on the test data\n",
    "    predictions = list()\n",
    "    for i in range(len(test_scaled)):\n",
    "        # make one-step forecast\n",
    "        X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "        yhat = forecast_lstm(lstm_model, 1, X)\n",
    "        # invert scaling\n",
    "        yhat = invert_scale(scaler, X, yhat)\n",
    "        # invert differencing\n",
    "        yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "        # store forecast\n",
    "        predictions.append(yhat)\n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "    print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "    error_scores.append(rmse)\n",
    " \n",
    "# summarize results\n",
    "results = pd.DataFrame()\n",
    "results['rmse'] = error_scores\n",
    "print(results.describe())\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcasting/Time Series\n",
    " - Inclusion of endog (target) variable into predictive model/forcast\n",
    " - Explore various techniques outlined statsmodels.pdf\n",
    " - Try LSTM neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.prev_item_move_date_int.values, X_train.prev_visit_date_int.values, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = df[ df.address1 == 'SPEEDWAY #1224']\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(foo.visit_date.values, foo.qty_shrink.values, c=foo.item_UPC.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_plot_sales():\n",
    "    freq = '2w'\n",
    "    item_filt = df.groupby(['item_category', pd.Grouper(key='visit_date', freq=freq)]).mean().reset_index()\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    i = 1\n",
    "    for cat in item_filt.item_category.unique():\n",
    "        if cat == '41':\n",
    "            continue\n",
    "        foo = item_filt[ item_filt.item_category == cat]\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(foo.visit_date, foo.sales_value, label=cat)\n",
    "        ax.set_xticklabels(foo.visit_date, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Average Lost Sales/{} ($)'.format(freq))\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title('Average Sales Loss Across All Stores by Cat')\n",
    "    \n",
    "cat_plot_sales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_plot_shrink():\n",
    "    freq = '2w'\n",
    "    item_filt = df.groupby(['item_category', pd.Grouper(key='visit_date', freq=freq)]).mean().reset_index()\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    i = 1\n",
    "    for cat in item_filt.item_category.unique():\n",
    "        if cat == '41':\n",
    "            continue\n",
    "        foo = item_filt[ item_filt.item_category == cat]\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(foo.visit_date, foo.qty_shrink, label=cat)\n",
    "        ax.set_xticklabels(foo.visit_date, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Average Shrink/{} ($)'.format(freq))\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title('Average Shrink Loss Across All Stores by Cat')\n",
    "    \n",
    "    \n",
    "cat_plot_shrink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def st_plot_sales():\n",
    "    freq = '20d'\n",
    "    item_filt = df.groupby(['state', pd.Grouper(key='visit_date', freq=freq)]).mean().reset_index()\n",
    "    fig = plt.figure(figsize=(40,60))\n",
    "    i = 1\n",
    "    count = 0\n",
    "    for state in item_filt.state.unique():\n",
    "        count += 1\n",
    "        if count % 9 == 0:\n",
    "            i += 1\n",
    "        foo = item_filt[ item_filt.state == state]\n",
    "        ax = fig.add_subplot(3,2,i)\n",
    "        ax.plot(foo.visit_date, foo.sales_value, label=state)\n",
    "        ax.set_xticklabels(foo.visit_date, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.set_ylabel('Average Lost Sales/{} ($)'.format(freq))\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_title('Average Sales Loss Across Cat by State')\n",
    "    \n",
    "st_plot_sales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_plot_sales():\n",
    "    freq = '2w'\n",
    "    item_filt = df.groupby(['customer_id', pd.Grouper(key='visit_date', freq=freq)]).mean().reset_index()\n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "    plt.tight_layout()\n",
    "    i = 1\n",
    "    for cust in item_filt.customer_id.unique():\n",
    "        if cust == '2741156':\n",
    "            continue\n",
    "        foo = item_filt[ item_filt.customer_id == cust]\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.plot(foo.visit_date, foo.sales_value, label=cust)\n",
    "        #ax.set_xticklabels(foo.visit_date, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Average Lost Sales/{} ($)'.format(freq))\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.title('Average Sales Loss Across All Stores by Customer')\n",
    "    #plt.yscale('log')\n",
    "    \n",
    "    \n",
    "cust_plot_sales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.scatter(df.visit_date.values, df.shrink_value.values, alpha=0.2)\n",
    "plt.yscale('log')\n",
    "plt.yticks([0.1,1,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['state', pd.Grouper(key='visit_date', freq='w')]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def st_plot_shrink():\n",
    "    freq = '20d'\n",
    "    item_filt = df.groupby(['state', pd.Grouper(key='visit_date', freq=freq)]).mean().reset_index()\n",
    "    fig = plt.figure(figsize=(40,60))\n",
    "    i = 1\n",
    "    count = 0\n",
    "    for state in item_filt.state.unique():\n",
    "        count += 1\n",
    "        if count % 9 == 0:\n",
    "            i += 1\n",
    "        foo = item_filt[ item_filt.state == state]\n",
    "        ax = fig.add_subplot(3,2,i)\n",
    "        ax.plot(foo.visit_date, foo.qty_shrink, label=state)\n",
    "        ax.set_xticklabels(foo.visit_date, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.set_ylabel('Average Shrink/{} ($)'.format(freq))\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_title('Average Shrink Loss Across Cat by State')\n",
    "    \n",
    "st_plot_shrink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_pickle('data/Crime/crime_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crime_plot():\n",
    "    freq = 'w'\n",
    "    item_filt = crime.groupby([pd.Grouper(key='date', freq=freq)]).count().reset_index()\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.plot(item_filt.date, item_filt.city)\n",
    "    plt.grid(alpha=0.4)\n",
    "crime_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(X_train, y, batch_size, nb_epoch, neurons):\n",
    "    X = X_train[]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_lstm(X_train, y, None, 1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. Why are there two salesman ID columns?\n",
    "2. Which columns are unknown (ie anything inventory out or equivalent)?\n",
    "3. Target is qty_shrink?\n",
    "4. What does customer_id represent? It has more values than address1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Data Sources\n",
    " - Crime data\n",
    " - Food desserts (people that may rely on gas stations for food)\n",
    " - Average income\n",
    " - Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.DataFrame()\n",
    "foo[['a', 'b']] = df[['visit_date', 'address1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POA\n",
    "- Create averages:\n",
    "    - Avg qty shrink/day, shink_sales/day, etc\n",
    "- Engineer lag terms (ie last visit, last month, last season)\n",
    "    - Use these in whatever model I want\n",
    "    - Use the averaged values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
