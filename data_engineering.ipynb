{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_int(row, col, df):\n",
    "    index = df.columns.get_loc(col)\n",
    "    date = row[index]\n",
    "    return time.mktime(time.strptime(str(date), '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type(df):\n",
    "    # datetime and date to int\n",
    "    date_cols = ['visit_date', 'prev_visit_date', 'prev_item_move_date', \n",
    "                 'last_edit_date', 'creation_date']\n",
    "    for col in date_cols:\n",
    "        # convert multiple time formats into single string format\n",
    "        df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # make time features specific data type in order to distinguish from other numberic values\n",
    "        df['{}_int'.format(col)] = df.apply(date_to_int, col=col, df=df, axis=1).astype(np.float32)\n",
    "        # convert string format back into datetime\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # objects\n",
    "    obj_cols = ['ship_id', 'address1', 'customer_id', 'sales_rep_id', 'item_id', 'old_item_id', \n",
    "                'item_UPC', 'old_item_UPC', 'ship_list_pk', 'sales_rep_id_2', 'list_header_id']\n",
    "    for col in obj_cols:\n",
    "        df[col] = df[col].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans(df):\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    # caution: crime data cuts total data in ~1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_code_inc(row, df):\n",
    "    index = df.columns.get_loc('postal_code')\n",
    "    code = row[index]\n",
    "    return int(code[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between_visits(df):\n",
    "    out_arr = []\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        diff = row['visit_date'] - row['prev_visit_date']\n",
    "        out_arr.append(pd.Timedelta(diff).days)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "def get_masked_df(df, level_list):\n",
    "    for ff_val in df[first_filter].unique():\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            for tf_val in df[ ff_mask & sf_mask][third_filter].unique():\n",
    "                tf_mask = df[third_filter] == tf_val\n",
    "                foo = df[ ff_mask & sf_mask & tf_mask ].sort_values('visit_date', ascending=False)\n",
    "    return df[ mask ].sort_values('visit_date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "def lag3(df, num_periods=1, first_filter='address1', second_filter='item_category', third_filter='item_upc'):\n",
    "    # initialize column(s) of nans\n",
    "    for period in range(1, num_periods + 1):\n",
    "        if period == 1:\n",
    "            continue\n",
    "        df['qty_shrink_per_day_lag{}'.format(period)] = np.nan\n",
    "        df['shrink_value_per_day_lag{}'.format(period)] = np.nan\n",
    "        \n",
    "    j = 0\n",
    "    for ff_val in df[first_filter].unique():\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print('another 10%...')\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            for tf_val in df[ ff_mask & sf_mask][third_filter].unique():\n",
    "                tf_mask = df[third_filter] == tf_val\n",
    "                foo = df[ ff_mask & sf_mask & tf_mask ].sort_values('visit_date', ascending=False)\n",
    "                #print('foo: ',foo[['visit_date']])\n",
    "                length = len(foo.visit_date.unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "                for period in range(1, num_periods + 1):\n",
    "                    if period == 1:\n",
    "                        continue\n",
    "                    # skip if there's not enough data to create lag variables\n",
    "                    filt_list = [first_filter, second_filter, third_filter]\n",
    "                    end_of_filters = 0\n",
    "                    while (length < period + 1) | end_of_filters:\n",
    "                        foo = get_masked_df(df, filt_list)\n",
    "                        # length = len(foo.visit_date.unique())\n",
    "                        filt_list.pop()\n",
    "                        continue\n",
    "                    #print('length: ',length)\n",
    "                    i = 0\n",
    "                    foo_shifted = foo.shift(-period)\n",
    "                    #print('fs: ', foo_shifted[['visit_date']])\n",
    "                    foo_grouped = foo.groupby('visit_date').mean()\n",
    "                    #print('fg: ', foo_shifted[['visit_date']])\n",
    "                    for index, row in foo.iterrows():\n",
    "                        #print(index)\n",
    "                        date = foo_shifted[ foo_shifted.index == index].visit_date.values[0]\n",
    "                        qty = foo_grouped[ foo_grouped.index == date].qty_shrink_per_day.values[0]\n",
    "                        value = foo_grouped[ foo_grouped.index == date].shrink_value_per_day.values[0]\n",
    "                        df.set_value(index, 'qty_shrink_per_day_lag{}'.format(period), qty)\n",
    "                        df.set_value(index, 'shrink_value_per_day_lag{}'.format(period), value)\n",
    "                        i += 1\n",
    "                        #print(i)\n",
    "                        if i + period == length:\n",
    "                            break # back to cat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(df, num_periods=1, first_filter='address1', second_filter='item_category', date_filter='visit_date',\n",
    "        lag_var1='qty_shrink_per_day', lag_var2='shrink_value_per_day', col_name_suf=''):\n",
    "    \n",
    "    # initialize column(s) of nans\n",
    "    for period in range(1, num_periods + 1):\n",
    "        df['{}_lag{}{}'.format(lag_var1, period, col_name_suf)] = np.nan\n",
    "        df['{}_lag{}{}'.format(lag_var2, period, col_name_suf)] = np.nan\n",
    "        \n",
    "    for ff_val in df[first_filter].unique():\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            foo = df[ ff_mask & sf_mask ].sort_values(date_filter, ascending=False)\n",
    "            #print('foo: ',foo[['visit_date']])\n",
    "            length = len(foo[date_filter].unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "            for period in range(1, num_periods + 1):\n",
    "                # skip if there's not enough data to create lag variables\n",
    "                if length < period + 1:\n",
    "                    continue\n",
    "                i = 0\n",
    "                foo_shifted = foo.shift(-period)\n",
    "                foo_grouped = foo.groupby(date_filter).mean()\n",
    "                for index, row in foo.iterrows():\n",
    "                    date = foo_shifted[ foo_shifted.index == index][date_filter].values[0]\n",
    "                    lag1_val = foo_grouped[ foo_grouped.index == date][lag_var1].values[0]\n",
    "                    lag2_val = foo_grouped[ foo_grouped.index == date][lag_var2].values[0]\n",
    "                    \n",
    "                    # set values\n",
    "                    df.set_value(index, '{}_lag{}{}'.format(lag_var1, period, col_name_suf), lag1_val)\n",
    "                    df.set_value(index, '{}_lag{}{}'.format(lag_var2, period, col_name_suf), lag2_val)\n",
    "                    i += 1\n",
    "                    if i + period == length:\n",
    "                        break # back to cat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_old(df, num_periods=1):\n",
    "    df['qty_shrink_per_day_lag1'] = np.nan\n",
    "    df['shrink_value_per_day_lag1'] = np.nan\n",
    "    j = 0\n",
    "    for add in df.address1.unique():\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print('another 10%...')\n",
    "        add_mask = df.address1 == add\n",
    "        for cat in df[add_mask].item_category.unique():\n",
    "            cat_mask = df.item_category == cat\n",
    "            foo = df[ add_mask & cat_mask].sort_values('visit_date', ascending=False)\n",
    "            #print('foo: ',foo[['visit_date']])\n",
    "            length = len(foo.visit_date.unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "            # skip if there's not enough data to create lag variables\n",
    "            if length < num_periods + 1:\n",
    "                continue\n",
    "            #print(length)\n",
    "            i = 1\n",
    "            foo_shifted = foo.shift(-num_periods)\n",
    "            #print('fs: ', foo_shifted[['visit_date']])\n",
    "            foo_grouped = foo.groupby('visit_date').mean()\n",
    "            for index, row in foo.iterrows():\n",
    "                #print(index)\n",
    "                date = foo_shifted[ foo_shifted.index == index].visit_date.values[0]\n",
    "                qty = foo_grouped[ foo_grouped.index == date].qty_shrink_per_day.values[0]\n",
    "                value = foo_grouped[ foo_grouped.index == date].shrink_value_per_day.values[0]\n",
    "                df.set_value(index, 'qty_shrink_per_day_lag1', qty)\n",
    "                df.set_value(index, 'shrink_value_per_day_lag1', value)\n",
    "                i += 1\n",
    "                #print(i)\n",
    "                if i == length:\n",
    "                    break # back to cat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(df, lag_periods):\n",
    "    df['zip_code'] = df.apply(zip_code_inc, df=df, axis=1)\n",
    "    \n",
    "    # normalize target variables\n",
    "    days_list = days_between_visits(df)\n",
    "    df['qty_shrink_per_day'] = df.qty_shrink / days_list\n",
    "    df['shrink_value_per_day'] = df.shrink_value / days_list\n",
    "    \n",
    "    # add lag variables\n",
    "    lag(df, num_periods=lag_periods) # caution: takes a long time\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(df):\n",
    "    del df['address3'] # redundant info (same as address 2)\n",
    "    del df['postal_code'] # create zip code\n",
    "    del df['duration'] # all zero values\n",
    "    del df['dist_customer_id'] # all -1 values\n",
    "    del df['POG_version_timestamp'] # dup of visit_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(df):\n",
    "    dummy_cols = ['item_category', 'customer_id']\n",
    "    foo = pd.DataFrame()\n",
    "    foo[dummy_cols] = df[dummy_cols].astype(str)\n",
    "    df = pd.get_dummies(df, columns=dummy_cols)\n",
    "    df[dummy_cols] = foo[dummy_cols]\n",
    "    del foo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file, lag_periods, remove_nan_rows=True):\n",
    "    df = pd.read_pickle(file)\n",
    "    data_type(df)\n",
    "    df = create(df, lag_periods)\n",
    "    drop(df)\n",
    "    df = dummy(df)\n",
    "    if remove_nan_rows:\n",
    "        nans(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:29: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# get SRP data and clean\n",
    "df = clean(file='data/SRP/raw_subset_300k.pkl', lag_periods=2, remove_nan_rows=True)\n",
    "# lag of 2 seems to be best bet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/SRP/clean_data_no_public.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/SRP/clean_data_no_public.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(df, include_crime=True, remove_nan_rows=True):\n",
    "    # load data\n",
    "    fd = pd.read_pickle('data/Food_Deserts/FD_clean.pkl').set_index('Zip Code')\n",
    "    unemp = pd.read_pickle('data/Unemployment/unemp_clean.pkl').set_index('Zip')\n",
    "    #inc = pd.read_pickle('data/Income/income_clean.pkl').set_index('ZIPCODE')\n",
    "    dens = pd.read_pickle('data/Pop_Density/density_clean.pkl').set_index('Zip/ZCTA')\n",
    "    crime = pd.read_pickle('data/Crime/grouped_clean.pkl').set_index(['state', 'city'])\n",
    "\n",
    "    # join via zip code\n",
    "    df = df.join(fd, on=['zip_code'], how='left')\n",
    "    df = df.join(unemp, on=['zip_code'], how='left')\n",
    "    # df = df.join(inc, on=['zip_code'], how='left')\n",
    "    df = df.join(dens, on=['zip_code'], how='left')\n",
    "    df['dens_sq_mile'] = df['dens/sq_mile'].replace(0, np.nan)\n",
    "    del df['dens/sq_mile']\n",
    "    \n",
    "    # join via city/state\n",
    "    if include_crime:\n",
    "        df = df.join(crime, on=['state', 'city'], how='left')\n",
    "        \n",
    "    # drop all rows that contain nan\n",
    "    if remove_nan_rows:\n",
    "        nans(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_code_str(row, df):\n",
    "    index = df.columns.get_loc('zip_code')\n",
    "    code = row[index]\n",
    "    return str(code).zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_data(df, include_crime=True, remove_nan_rows=True)\n",
    "df['zip_code'] = df.apply(zip_code_str, df=df, axis=1)\n",
    "\n",
    "# still need to impute nans and 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/SRP/clean_data_public_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POA\n",
    "- Create averages:\n",
    "    - Avg qty shrink/day, shink_sales/day, etc\n",
    "- Engineer lag terms (ie last visit, last month, last season)\n",
    "    - Use these in whatever model I want\n",
    "    - Use the averaged values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
