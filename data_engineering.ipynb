{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('max_columns',500)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_int(row, col, df):\n",
    "    index = df.columns.get_loc(col)\n",
    "    date = row[index]\n",
    "    return time.mktime(time.strptime(str(date), '%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type(df):\n",
    "    # datetime and date to int\n",
    "    date_cols = ['visit_date', 'prev_visit_date', 'prev_item_move_date', \n",
    "                 'last_edit_date', 'creation_date']\n",
    "    for col in date_cols:\n",
    "        # convert multiple time formats into single string format\n",
    "        df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # make time features specific data type in order to distinguish from other numberic values\n",
    "        df['{}_int'.format(col)] = df.apply(date_to_int, col=col, df=df, axis=1).astype(np.float32)\n",
    "        # convert string format back into datetime\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # objects\n",
    "    obj_cols = ['ship_id', 'address1', 'customer_id', 'sales_rep_id', 'item_id', 'old_item_id', \n",
    "                'item_UPC', 'old_item_UPC', 'ship_list_pk', 'sales_rep_id_2', 'list_header_id']\n",
    "    for col in obj_cols:\n",
    "        df[col] = df[col].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans(df):\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    # caution: crime data cuts total data in ~1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_code_inc(row, df):\n",
    "    index = df.columns.get_loc('postal_code')\n",
    "    code = row[index]\n",
    "    return int(code[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between_visits(df):\n",
    "    out_arr = []\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        diff = row['visit_date'] - row['prev_visit_date']\n",
    "        out_arr.append(pd.Timedelta(diff).days)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "def get_masked_df(df, level_list):\n",
    "    for ff_val in df[first_filter].unique():\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            for tf_val in df[ ff_mask & sf_mask][third_filter].unique():\n",
    "                tf_mask = df[third_filter] == tf_val\n",
    "                foo = df[ ff_mask & sf_mask & tf_mask ].sort_values('visit_date', ascending=False)\n",
    "    return df[ mask ].sort_values('visit_date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "def lag3(df, num_periods=1, first_filter='address1', second_filter='item_category', third_filter='item_upc'):\n",
    "    # initialize column(s) of nans\n",
    "    for period in range(1, num_periods + 1):\n",
    "        if period == 1:\n",
    "            continue\n",
    "        df['qty_shrink_per_day_lag{}'.format(period)] = np.nan\n",
    "        df['shrink_value_per_day_lag{}'.format(period)] = np.nan\n",
    "        \n",
    "    j = 0\n",
    "    for ff_val in df[first_filter].unique():\n",
    "        j += 1\n",
    "        if j % 1000 == 0:\n",
    "            print('another 10%...')\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            for tf_val in df[ ff_mask & sf_mask][third_filter].unique():\n",
    "                tf_mask = df[third_filter] == tf_val\n",
    "                foo = df[ ff_mask & sf_mask & tf_mask ].sort_values('visit_date', ascending=False)\n",
    "                #print('foo: ',foo[['visit_date']])\n",
    "                length = len(foo.visit_date.unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "                for period in range(1, num_periods + 1):\n",
    "                    if period == 1:\n",
    "                        continue\n",
    "                    # skip if there's not enough data to create lag variables\n",
    "                    filt_list = [first_filter, second_filter, third_filter]\n",
    "                    end_of_filters = 0\n",
    "                    while (length < period + 1) | end_of_filters:\n",
    "                        foo = get_masked_df(df, filt_list)\n",
    "                        # length = len(foo.visit_date.unique())\n",
    "                        filt_list.pop()\n",
    "                        continue\n",
    "                    #print('length: ',length)\n",
    "                    i = 0\n",
    "                    foo_shifted = foo.shift(-period)\n",
    "                    #print('fs: ', foo_shifted[['visit_date']])\n",
    "                    foo_grouped = foo.groupby('visit_date').mean()\n",
    "                    #print('fg: ', foo_shifted[['visit_date']])\n",
    "                    for index, row in foo.iterrows():\n",
    "                        #print(index)\n",
    "                        date = foo_shifted[ foo_shifted.index == index].visit_date.values[0]\n",
    "                        qty = foo_grouped[ foo_grouped.index == date].qty_shrink_per_day.values[0]\n",
    "                        value = foo_grouped[ foo_grouped.index == date].shrink_value_per_day.values[0]\n",
    "                        df.set_value(index, 'qty_shrink_per_day_lag{}'.format(period), qty)\n",
    "                        df.set_value(index, 'shrink_value_per_day_lag{}'.format(period), value)\n",
    "                        i += 1\n",
    "                        #print(i)\n",
    "                        if i + period == length:\n",
    "                            break # back to cat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nans(df, num_periods, lag_vars, col_name_suf):\n",
    "    for period in range(1, num_periods + 1):\n",
    "        for lag_var in lag_vars:\n",
    "            df['{}_lag{}{}'.format(lag_var, period, col_name_suf)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_rec(df, num_periods, col_filters, date_col, lag_vars, col_name_suf, mask=True):\n",
    "    '''\n",
    "    Recursively loop through various heirarchaly ordered columns, grouping by date\n",
    "    INPUTS:\n",
    "        df - pandas dataframe\n",
    "        num_periods - number of periods (previous dates) to go back and attempt to fill lag values for\n",
    "        col_filters - columns to heiracrchally filter down on, with the last column being the one ultimately used\n",
    "        date_col - date column to use in grouping and lag periods\n",
    "        col_name_suf - suffix to append to newly created columns (help to distinguish between last filtered column choosen)\n",
    "        mask - DO NOT CHANGE. Required to be True to maintain dataframe mask between recursive iterations\n",
    "    Returns:\n",
    "        Nothing. All dataframe column changes occur in place\n",
    "    '''\n",
    "    # begin with mask of all trues\n",
    "    true_mask = np.ones(len(df), dtype=bool)\n",
    "    loop_mask = mask & true_mask\n",
    "    col_filter = col_filters[0]\n",
    "    for val in df[ loop_mask ][col_filter].unique():\n",
    "        val_mask = df[col_filter] == val\n",
    "        comb_mask = loop_mask & val_mask\n",
    "\n",
    "        if len(col_filters) > 1:\n",
    "            #recursively update the remaining items' positions\n",
    "            lag_rec(df, num_periods, col_filters[1:], date_col, lag_vars, col_name_suf, mask=comb_mask)\n",
    "        else:\n",
    "            foo = df[ comb_mask ].sort_values(date_col, ascending=False)     \n",
    "            length = len(foo[date_col].unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "            for period in range(1, num_periods + 1):\n",
    "                # skip if there's not enough data to create lag variables\n",
    "                if length < period + 1:\n",
    "                    continue\n",
    "                i = 0\n",
    "                # create duplicate df, but with all indices shifted by the current 'period' number\n",
    "                foo_shifted = foo.shift(-period)\n",
    "                foo_grouped = foo.groupby(date_col).mean()\n",
    "                for index, row in foo.iterrows():\n",
    "                    date = foo_shifted[ foo_shifted.index == index ][date_col].values[0]\n",
    "                    for lag_var in lag_vars:\n",
    "                        lag_val = foo_grouped[ foo_grouped.index == date ][lag_var].values[0]\n",
    "                        # set value\n",
    "                        df.set_value(index, '{}_lag{}{}'.format(lag_var, period, col_name_suf), lag_val)\n",
    "                    i += 1\n",
    "                    if i + period == length:\n",
    "                        break # back to period loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(df, num_periods, col_filters, date_filter, lag_vars, col_name_suf):\n",
    "    init_nans(df, num_periods, lag_vars, col_name_suf)\n",
    "    lag_rec(df, num_periods, col_filters, date_filter, lag_vars, col_name_suf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_old(df, num_periods=1, first_filter='address1', second_filter='item_category', date_filter='visit_date',\n",
    "        lag_var1='qty_shrink_per_day', lag_var2='shrink_value_per_day', col_name_suf=''):\n",
    "    \n",
    "    # initialize column(s) of nans\n",
    "    for period in range(1, num_periods + 1):\n",
    "        df['{}_lag{}{}'.format(lag_var1, period, col_name_suf)] = np.nan\n",
    "        df['{}_lag{}{}'.format(lag_var2, period, col_name_suf)] = np.nan\n",
    "        \n",
    "    for ff_val in df[first_filter].unique():\n",
    "        ff_mask = df[first_filter] == ff_val\n",
    "        for sf_val in df[ff_mask][second_filter].unique():\n",
    "            sf_mask = df[second_filter] == sf_val\n",
    "            foo = df[ ff_mask & sf_mask ].sort_values(date_filter, ascending=False)\n",
    "            length = len(foo[date_filter].unique()) # determine number of visits (because multiple item categories can be updated in a single visit)\n",
    "            for period in range(1, num_periods + 1):\n",
    "                # skip if there's not enough data to create lag variables\n",
    "                if length < period + 1:\n",
    "                    continue\n",
    "                i = 0\n",
    "                foo_shifted = foo.shift(-period)\n",
    "                foo_grouped = foo.groupby(date_filter).mean()\n",
    "                for index, row in foo.iterrows():\n",
    "                    date = foo_shifted[ foo_shifted.index == index][date_filter].values[0]\n",
    "                    lag1_val = foo_grouped[ foo_grouped.index == date][lag_var1].values[0]\n",
    "                    lag2_val = foo_grouped[ foo_grouped.index == date][lag_var2].values[0]\n",
    "                    \n",
    "                    # set values\n",
    "                    df.set_value(index, '{}_lag{}{}'.format(lag_var1, period, col_name_suf), lag1_val)\n",
    "                    df.set_value(index, '{}_lag{}{}'.format(lag_var2, period, col_name_suf), lag2_val)\n",
    "                    i += 1\n",
    "                    if i + period == length:\n",
    "                        break # back to cat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(df, num_periods, col_filters, date_filter, lag_vars, col_name_suf):\n",
    "    df['zip_code'] = df.apply(zip_code_inc, df=df, axis=1)\n",
    "    \n",
    "    # normalize target variables\n",
    "    days_list = days_between_visits(df)\n",
    "    df['qty_shrink_per_day'] = df.qty_shrink / days_list\n",
    "    df['shrink_value_per_day'] = df.shrink_value / days_list\n",
    "    \n",
    "    # add lag variables\n",
    "    df = lag(df, num_periods, col_filters, date_filter, lag_vars, col_name_suf) # caution: takes a long time\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(df):\n",
    "    del df['address3'] # redundant info (same as address 2)\n",
    "    del df['postal_code'] # create zip code\n",
    "    del df['duration'] # all zero values\n",
    "    del df['dist_customer_id'] # all -1 values\n",
    "    del df['POG_version_timestamp'] # dup of visit_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(df):\n",
    "    dummy_cols = ['item_category', 'customer_id']\n",
    "    foo = pd.DataFrame()\n",
    "    foo[dummy_cols] = df[dummy_cols].astype(str)\n",
    "    df = pd.get_dummies(df, columns=dummy_cols)\n",
    "    df[dummy_cols] = foo[dummy_cols]\n",
    "    del foo\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file, lag_periods, col_filters, date_filter, lag_vars, col_name_suf, remove_nan_rows=True):\n",
    "    df = pd.read_pickle(file)\n",
    "    data_type(df)\n",
    "    df = create(df, lag_periods, col_filters, date_filter, lag_vars, col_name_suf)\n",
    "    drop(df)\n",
    "    df = dummy(df)\n",
    "    if remove_nan_rows:\n",
    "        nans(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/SRP/raw_mini_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:41: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# get SRP data and clean\n",
    "df = clean(file='data/SRP/raw_subset_300k.pkl', lag_periods=3, col_filters=['address1'], date_filter='visit_date',\n",
    "        lag_vars=['qty_shrink_per_day', 'shrink_value_per_day'], col_name_suf='_by_store', remove_nan_rows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/SRP/clean_data_no_public_lag3_by_store.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/SRP/clean_data_no_public_lag3_by_store.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(df, include_crime=True, remove_nan_rows=True):\n",
    "    # load data\n",
    "    fd = pd.read_pickle('data/Food_Deserts/FD_clean.pkl').set_index('Zip Code')\n",
    "    unemp = pd.read_pickle('data/Unemployment/unemp_clean.pkl').set_index('Zip')\n",
    "    #inc = pd.read_pickle('data/Income/income_clean.pkl').set_index('ZIPCODE')\n",
    "    dens = pd.read_pickle('data/Pop_Density/density_clean.pkl').set_index('Zip/ZCTA')\n",
    "    crime = pd.read_pickle('data/Crime/grouped_clean.pkl').set_index(['state', 'city'])\n",
    "\n",
    "    # join via zip code\n",
    "    df = df.join(fd, on=['zip_code'], how='left')\n",
    "    df = df.join(unemp, on=['zip_code'], how='left')\n",
    "    # df = df.join(inc, on=['zip_code'], how='left')\n",
    "    df = df.join(dens, on=['zip_code'], how='left')\n",
    "    df['dens_sq_mile'] = df['dens/sq_mile'].replace(0, np.nan)\n",
    "    del df['dens/sq_mile']\n",
    "    \n",
    "    # join via city/state\n",
    "    if include_crime:\n",
    "        df = df.join(crime, on=['state', 'city'], how='left')\n",
    "        \n",
    "    # drop all rows that contain nan\n",
    "    if remove_nan_rows:\n",
    "        nans(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_code_str(row, df):\n",
    "    index = df.columns.get_loc('zip_code')\n",
    "    code = row[index]\n",
    "    return str(code).zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_data(df, include_crime=True, remove_nan_rows=True)\n",
    "df['zip_code'] = df.apply(zip_code_str, df=df, axis=1)\n",
    "\n",
    "# still need to impute nans and 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/SRP/clean_data_public_all_lag3_by_store.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POA\n",
    "- Create averages:\n",
    "    - Avg qty shrink/day, shink_sales/day, etc\n",
    "- Engineer lag terms (ie last visit, last month, last season)\n",
    "    - Use these in whatever model I want\n",
    "    - Use the averaged values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
